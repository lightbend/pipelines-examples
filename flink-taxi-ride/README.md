## Flink based Pipelines Application

### Problem Definition

We work with two data streams, one with `TaxiRide` events generated by a Akka stream streamlet (ingress) and the other with `TaxiFare` events generated by another Akka stream streamlet (ingress). The 2 streams are then connected together through a Flink streamlet based processor which does a stateful enrichment that builds up an aggregate of `TaxiRide` to `TaxiFare` mappings.

The mapping is then posted on to an Akka stream streamlet (egress) as a tuple.

### Sub projects

The following sub-projects constitute the whole application:

* `datamodel` - contains the Avro schema for `TaxiRide`, `TaxiFare` and `TaxiRideFare`
* `ingestor` - contains the Akka stream ingresses that read data streams from http
* `processor` - the Flink streamlet that connects the input streams and does stateful processing to generate the output stream
* `logger` - contains the Akka stream egress that writes to Kafka. The logger streamlet has the following configurable parameters:
  * `valid-logger.log-level` - Log level for `*-logger` streamlets to log to. e.g. `info`
  * `valid-logger.msg-prefix` - Log line prefix for `*-logger` streamlets to include. e.g. `VALID`
* `taxi-ride-pipeline` - the entry point containing the blueprint definition

### Build the application

Here's the sequence of steps that you need to follow:

```
$ pwd
.../flink-taxi-ride
$ sbt
$ clean
$ buildAndPublish
```

The above will build the application and publish application Docker images to the Docker registry, as configured in `target-env.sbt`.

> **Note:** You need to copy `target-env.sbt.example` to `target-env.sbt` with appropriate settings for the Docker registry in order for the build and publish to go through.

The `buildAndPublish` command, if successful, will publish the exact command to use for deployment in the cluster.

### Feeding data into the application

The project comes with scripts that can be used to feed data into the ingresses using http. 

The folder `test-data` contains 2 bash scripts, `send-data-rides.sh` and `send-data-fares.sh` that can be used to feed data through http to the 2 ingresses. You need to change the cluster names in the scripts to match your own environment.

